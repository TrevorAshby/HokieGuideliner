{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 512\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\")\n",
    "\n",
    "blen_tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot-400M-distill\")\n",
    "blen_model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/blenderbot-400M-distill\")\n",
    "\n",
    "inst_tokenizer = AutoTokenizer.from_pretrained(\"prakharz/DIAL-BART0\")\n",
    "inst_model = AutoModelForSeq2SeqLM.from_pretrained(\"prakharz/DIAL-BART0\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the trained checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): Embedding(50265, 1024, padding_idx=1)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50265, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./model/guideliner.pt'))\n",
    "blen_model.load_state_dict(torch.load('./model/blenderbot.pt'))\n",
    "inst_model.load_state_dict(torch.load('./model/intructdialogue.pt'))\n",
    "\n",
    "model.eval()\n",
    "blen_model.eval()\n",
    "inst_model.eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate from the Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When generating the guideline, make sure you follow the below format:\n",
    "```\n",
    "<Conversation history where A is the human and B is the robot> | <topic prefences> \n",
    "```\n",
    "*Notice that they are separated by the '|' character*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The generated guideline is:  The user is a big fan of Harry Potter and has read all five books in the series. Ask them about their favorite book and suggest similar books they might enjoy.\n"
     ]
    }
   ],
   "source": [
    "# Set the input string\n",
    "in_str = \"A: Oh, I've read \\\"1984\\\". It\\'s a classic. Love it. Have you ever read anything by J.K. Rowling? \\\n",
    "    B: Yes, I love the Harry Potter series. Have you read them? A: Yes, I'm a big fan of Harry Potter. \\\n",
    "        I have read all the books and seen all the movies. Do you have a favorite book in the series?|\\\n",
    "            {\\\"high-level\\\": {\\\"topic\\\": \\\"literature\\\", \\\"if_interest\\\": \\\"yes\\\"}}, \\\n",
    "                {\\\"middle-level\\\": {\\\"topic\\\": \\\"book recommendation\\\", \\\"if_interest\\\": \\\"unknow\\\"}}, \\\n",
    "                    {\\\"low-level\\\": {\\\"topic\\\": \\\"J.K. Rowling\\\", \\\"if_interest\\\": \\\"unknow\\\"}} \\\n",
    "                        {\\\"high-level\\\": {\\\"topic\\\": \\\"literature\\\", \\\"if_interest\\\": \\\"yes\\\"}}, \\\n",
    "                            {\\\"middle-level\\\": {\\\"topic\\\": \\\"book recommendation\\\", \\\"if_interest\\\": \\\"yes\\\"}, \\\"low-level\\\": {\\\"topic\\\": \\\"Harry Potter\\\", \\\"if_interest\\\": \\\"yes\\\"}}\"\n",
    "\n",
    "# Generate the guideline based upon the history and the topic preferences\n",
    "in_ids = tokenizer(in_str, max_length=max_length, padding='max_length', return_tensors='pt').input_ids\n",
    "example = model.generate(in_ids, max_new_tokens=50)\n",
    "guideline = tokenizer.decode(example[0], skip_special_tokens=True)\n",
    "print(\"The generated guideline is: \", guideline)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BlenderBot Generation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When generating using the Blenderbot model, make sure to use the following format:\n",
    "```\n",
    "*ROBOT TEXT*</s> <s>*USER RESPONSE* [GUIDELINE] *GUIDELINE*\n",
    "or\n",
    "*ROBOT TEXT*</s> <s>*USER RESPONSE*</s> <s>*ROBOT TEXT*</s> <s>*USER RESPONSE [GUIDELINE] *GUIDELINE*\n",
    "```\n",
    "*Notice that each conversation turn by robot and user is separated by \"/s\" and \"s\" within <>*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InstructDialogue Generation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When generating using the Instructdialogue model, make sure to use the following format:\n",
    "```\n",
    "Instruction: *INSTRUCTION TEXT*\n",
    "\n",
    "Input: [TOPICS] *Text talking about the topic of conversation* [CONTEXT] *CONVERSATION HISTORY* [ENDOFDIALOGUE] [QUESTION] *QUESTION TEXT* [GUIDELINE] *GUIDELINE TEXT*\n",
    "\n",
    "```\n",
    "\n",
    "```python\n",
    "# There is a special format for the conversation history that must be followed. Below is an example\n",
    "\"Robot: Hi there, how are you doing today? Human: I'm doing pretty well. [ENDOFTURN] Robot: Same here. I love your earrings, where did you get them? Human: Thanks! I got them from a local boutique.\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.28441858291626 sec - Blenderbot generated the following response:  I think my favorite book is Order of the Phoenix. What's yours?\n",
      "15.568578481674194 sec - Instructdialogue generated the following response:  Robot: I've read \"1984\" and \"Harry Potter and the Order of the Phoenix\", but I haven't seen the movies yet. What's your favorite book in the series?\n"
     ]
    }
   ],
   "source": [
    "# modify in_str for each model\n",
    "blend_modified_in = in_str.split('|')[0]\n",
    "blend_modified_in = blend_modified_in[3:] # remove 'A:'\n",
    "blend_modified_in = blend_modified_in.replace('B: ', '</s> <s>')\n",
    "blend_modified_in = blend_modified_in.replace('A: ', '</s> <s>')\n",
    "\n",
    "#! NEED TO FIGURE OUT HOW TO INSERT CONVERSATION TURNS\n",
    "#inst_modified_in = in_str.split('|')[0]\n",
    "#inst_modified_in = inst_modified_in.replace('A:', 'Human:')\n",
    "#inst_modified_in = inst_modified_in.replace('B:', 'Robot:')\n",
    "inst_modified_in = \"Human: Oh, I've read \\\"1984\\\". It\\'s a classic. Love it. Have you ever read anything by J.K. Rowling? \\\n",
    "    Robot: Yes, I love the Harry Potter series. Have you read them? Human: Yes, I'm a big fan of Harry Potter. \\\n",
    "    I have read all the books and seen all the movies. Do you have a favorite book in the series?\"\n",
    "\n",
    "topics = \"These two people are talking about books.\"\n",
    "question = \"Given this conversation provided, a response following the topic guideline is\"\n",
    "\n",
    "# Set the input strings\n",
    "blend_in_str = blend_modified_in + \" [GUIDELINE] \" + guideline\n",
    "inst_in_str = \"Input: [TOPICS] \" + topics + \" [CONTEXT] \" + inst_modified_in + \" [ENDOFDIALOGUE] [QUESTION] \" + question + \" [GUIDELINE] \" + guideline\n",
    "\n",
    "\n",
    "# Generate the response using the blenderbot model\n",
    "blend_start = time.time()\n",
    "blend_in_ids = blen_tokenizer([blend_in_str], max_length=128, return_tensors='pt', truncation=True)\n",
    "blend_example = blen_model.generate(**blend_in_ids, max_length=60)\n",
    "blend_response = blen_tokenizer.batch_decode(blend_example, skip_special_tokens=True)[0]\n",
    "blend_time = (time.time() - blend_start)%60\n",
    "print(\"{} sec - Blenderbot generated the following response: {}\".format(blend_time, blend_response))\n",
    "\n",
    "# Generate the response using the instructdialogue model\n",
    "inst_start = time.time()\n",
    "inst_in_ids = inst_tokenizer(inst_in_str, max_length=128, return_tensors='pt', truncation=True)\n",
    "inst_example = inst_model.generate(**inst_in_ids, max_length=60)\n",
    "inst_response = inst_tokenizer.batch_decode(inst_example, skip_special_tokens=True)[0]\n",
    "inst_time = (time.time() - inst_start)%60\n",
    "print(\"{} sec - Instructdialogue generated the following response: {}\".format(inst_time, inst_response))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hokieguideliner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
