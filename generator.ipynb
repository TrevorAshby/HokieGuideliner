{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/trevor/SecondaryM2/VirginiaTech/HokieGuideliner/hokieguideliner/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm  # for our progress bar\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 1.15k/1.15k [00:00<00:00, 4.56MB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.57k/1.57k [00:00<00:00, 7.25MB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 127k/127k [00:00<00:00, 28.9MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 62.9k/62.9k [00:00<00:00, 1.24MB/s]\n",
      "Downloading (…)in/added_tokens.json: 100%|██████████| 16.0/16.0 [00:00<00:00, 69.0kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 772/772 [00:00<00:00, 3.54MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 730M/730M [00:08<00:00, 85.3MB/s] \n",
      "Downloading (…)neration_config.json: 100%|██████████| 347/347 [00:00<00:00, 1.70MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 383/383 [00:00<00:00, 1.90MB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 798k/798k [00:00<00:00, 4.99MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 8.58MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 2.11M/2.11M [00:00<00:00, 32.3MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 280/280 [00:00<00:00, 1.18MB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.75k/1.75k [00:00<00:00, 7.33MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 1.63G/1.63G [00:18<00:00, 85.7MB/s]\n"
     ]
    }
   ],
   "source": [
    "blen_tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot-400M-distill\")\n",
    "blen_model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/blenderbot-400M-distill\")\n",
    "\n",
    "inst_tokenizer = AutoTokenizer.from_pretrained(\"prakharz/DIAL-BART0\")\n",
    "inst_model = AutoModelForSeq2SeqLM.from_pretrained(\"prakharz/DIAL-BART0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_id = '0' # need to change this to 6 when I am training w/ jingyuan's GPU\n",
    "max_length = 512\n",
    "batch_size = 32\n",
    "epoch_num = 50\n",
    "\n",
    "device = torch.device('cuda:'+device_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.1299, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "UTTERANCE = \"My friends are cool but they eat too many carbs. </s> <s>That\\'s unfortunate. Are they trying to lose weight or are they just trying to be healthier?</s> <s>I'm not sure.\"\n",
    "LABEL = \"Well, maybe they are struggling?\"\n",
    "\n",
    "inputs = blen_tokenizer([UTTERANCE], return_tensors='pt').input_ids\n",
    "lab = blen_tokenizer([LABEL], return_tensors='pt').input_ids\n",
    "\n",
    "reply_ids = blen_model(input_ids=inputs, labels=lab)\n",
    "print(reply_ids.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I hope they can find a way to do it. I know it can be hard.\n"
     ]
    }
   ],
   "source": [
    "UTTERANCE = \"My friends are cool but they eat too many carbs. </s> <s>That\\'s unfortunate. Are they trying to lose weight or are they just trying to be healthier?</s> <s>I'm not sure.\"\n",
    "inputs = blen_tokenizer([UTTERANCE], return_tensors='pt')\n",
    "reply_ids = blen_model.generate(**inputs, max_new_tokens=50)\n",
    "print(blen_tokenizer.batch_decode(reply_ids, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_guideline(topics):\n",
    "    history = ''\n",
    "    if topics is None:\n",
    "        return history\n",
    "    if 'high-level' in topics and topics['high-level'] is not None and 'topic' in topics['high-level']:\n",
    "        if type(topics['high-level']['topic']) is list:\n",
    "            topic = ', '.join(topics['high-level']['topic'])\n",
    "        else: \n",
    "            topic = topics['high-level']['topic']\n",
    "        if topic is not None:\n",
    "            history += 'These two people are talking about ' + topic + '.'\n",
    "            if 'if_interest' in topics['high-level'] and topics['high-level']['if_interest'] == 'no':\n",
    "                history += ' However, user B does not like this topic.'\n",
    "            \n",
    "    if 'middle-level' in topics and topics['middle-level'] is not None and 'topic' in topics['middle-level']:\n",
    "        if type(topics['middle-level']['topic']) is list:\n",
    "            topic = ', '.join(topics['middle-level']['topic'])\n",
    "        else: \n",
    "            topic = topics['middle-level']['topic']\n",
    "        if topic is not None:\n",
    "            history += ' Specifically, they focus on ' + topic + '.'\n",
    "            if 'if_interest' in topics['middle-level'] and topics['middle-level']['if_interest'] == 'no':\n",
    "                history += ' However, user B does not like this.'\n",
    "            \n",
    "    if 'low-level' in topics and topics['low-level'] is not None and 'topic' in topics['low-level']:\n",
    "        if type(topics['low-level']['topic']) is list:\n",
    "            topic = ', '.join(topics['low-level']['topic'])\n",
    "        else: \n",
    "            topic = topics['low-level']['topic']\n",
    "        if topic is not None:\n",
    "            history += ' In more detail, they are discussing about ' + topic + '.'\n",
    "            if 'if_interest' in topics['low-level'] and topics['low-level']['if_interest'] == 'no':\n",
    "                history += ' However, user B does not want to talk about it.'\n",
    "            \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open('./sample_conv4.json', 'r'))\n",
    "train_data = data[:int(0.8*len(data))]\n",
    "train_input = []\n",
    "train_output = []\n",
    "sep = ' [ENDOFTURN] '\n",
    "instruction = 'Instruction: Generate a response that following the given topic guidline. \\n\\nInput: [TOPICS] '\n",
    "cont = ' [CONTEXT] '\n",
    "end = ' [ENDOFDIALOGUE] [QUESTION] Given this conversation provided, a response following the topic guideline is'\n",
    "for conversation in train_data:\n",
    "    dialogue_history = \"\"\n",
    "    for idx, message in enumerate(conversation):\n",
    "        if len(dialogue_history) != 0:\n",
    "            if 'topic_dict' not in message:\n",
    "                dialogue_history += sep\n",
    "            else:\n",
    "                dialogue_history += ' '\n",
    "                topic = get_topic_guideline(message['topic_dict'])\n",
    "        dialogue_history += message['message']\n",
    "        dialogue_history = dialogue_history.replace('A: ', 'Robot: ')\n",
    "        dialogue_history = dialogue_history.replace('B: ', 'Human: ')\n",
    "        \n",
    "        if 'topic_dict' in message:\n",
    "            input_text = instruction + topic + cont + dialogue_history + end\n",
    "            if message['topic_dict'] is not None and idx < len(conversation) - 1:\n",
    "                output = conversation[idx + 1]['message']\n",
    "                output = output.replace('A: ', 'Robot: ')\n",
    "                output = output.replace('B: ', 'Human: ')\n",
    "            \n",
    "                \n",
    "                train_input.append(input_text)\n",
    "                train_output.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12344 12344\n"
     ]
    }
   ],
   "source": [
    "text_input = [line.strip() for line in train_input]\n",
    "text_label = [line.strip() for line in train_output]\n",
    "print(len(text_input), len(text_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Generate a response that following the given topic guidline. \n",
      "\n",
      "Input: [TOPICS] These two people are talking about emotions. Specifically, they focus on well-being. [CONTEXT] Robot: Hi, how are you doing today? Human: I'm doing pretty well. [ENDOFDIALOGUE] [QUESTION] Given this conversation provided, a response following the topic guideline is\n",
      "Robot: That's good to hear. Have you heard the news about the upcoming election?\n",
      "---\n",
      "Instruction: Generate a response that following the given topic guidline. \n",
      "\n",
      "Input: [TOPICS] These two people are talking about news. However, user B does not like this topic. Specifically, they focus on politics. However, user B does not like this. [CONTEXT] Robot: Hi, how are you doing today? Human: I'm doing pretty well. [ENDOFTURN] Robot: That's good to hear. Have you heard the news about the upcoming election? Human: I have, but to be honest, I'm not too interested in politics. [ENDOFDIALOGUE] [QUESTION] Given this conversation provided, a response following the topic guideline is\n",
      "Robot: Yeah, I understand. Have you been watching any good shows or movies lately?\n",
      "---\n",
      "Instruction: Generate a response that following the given topic guidline. \n",
      "\n",
      "Input: [TOPICS] These two people are talking about entertainment. Specifically, they focus on documentaries. In more detail, they are discussing about space documentary. [CONTEXT] Robot: Hi, how are you doing today? Human: I'm doing pretty well. [ENDOFTURN] Robot: That's good to hear. Have you heard the news about the upcoming election? Human: I have, but to be honest, I'm not too interested in politics. [ENDOFTURN] Robot: Yeah, I understand. Have you been watching any good shows or movies lately? Human: Actually, I just finished a really interesting documentary about space. [ENDOFDIALOGUE] [QUESTION] Given this conversation provided, a response following the topic guideline is\n",
      "Robot: Sounds cool! Do you like space-themed video games too?\n",
      "---\n",
      "Instruction: Generate a response that following the given topic guidline. \n",
      "\n",
      "Input: [TOPICS] These two people are talking about games. However, user B does not like this topic. Specifically, they focus on video games. However, user B does not like this. [CONTEXT] Robot: Hi, how are you doing today? Human: I'm doing pretty well. [ENDOFTURN] Robot: That's good to hear. Have you heard the news about the upcoming election? Human: I have, but to be honest, I'm not too interested in politics. [ENDOFTURN] Robot: Yeah, I understand. Have you been watching any good shows or movies lately? Human: Actually, I just finished a really interesting documentary about space. [ENDOFTURN] Robot: Sounds cool! Do you like space-themed video games too? Human: I don't play video games much, to be honest. [ENDOFDIALOGUE] [QUESTION] Given this conversation provided, a response following the topic guideline is\n",
      "Robot: Oh, fair enough. What kind of music do you like then?\n",
      "---\n",
      "Instruction: Generate a response that following the given topic guidline. \n",
      "\n",
      "Input: [TOPICS] These two people are talking about music. Specifically, they focus on rock n roll. [CONTEXT] Robot: Hi, how are you doing today? Human: I'm doing pretty well. [ENDOFTURN] Robot: That's good to hear. Have you heard the news about the upcoming election? Human: I have, but to be honest, I'm not too interested in politics. [ENDOFTURN] Robot: Yeah, I understand. Have you been watching any good shows or movies lately? Human: Actually, I just finished a really interesting documentary about space. [ENDOFTURN] Robot: Sounds cool! Do you like space-themed video games too? Human: I don't play video games much, to be honest. [ENDOFTURN] Robot: Oh, fair enough. What kind of music do you like then? Human: I'm more of an oldies person, like classic rock and roll. [ENDOFDIALOGUE] [QUESTION] Given this conversation provided, a response following the topic guideline is\n",
      "Robot: Nice, any specific bands or artists?\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(text_input[i])\n",
    "    print(text_label[i])\n",
    "    print('---')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hokieguideliner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
