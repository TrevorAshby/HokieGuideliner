{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/trevor/SecondaryM2/VirginiaTech/HokieGuideliner/hokieguideliner/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, T5ForConditionalGeneration\n",
    "\n",
    "from transformers import AutoModel, AutoConfig\n",
    "\n",
    "from scipy.special import softmax\n",
    "import numpy as np\n",
    "\n",
    "# from peft import PeftModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Input / Memory Extraction 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "inst_tokenizer = AutoTokenizer.from_pretrained(\"prakharz/DIAL-BART0\")\n",
    "inst_model = AutoModelForSeq2SeqLM.from_pretrained(\"prakharz/DIAL-BART0\")\n",
    "\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "sent_tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "sent_model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "config = AutoConfig.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"I like country music.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/trevor/SecondaryM2/VirginiaTech/HokieGuideliner/hokieguideliner/lib/python3.10/site-packages/transformers/generation/utils.py:1346: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country music\n"
     ]
    }
   ],
   "source": [
    "instruct_input = \"Instruction:What is the topic of conversation?\\n\\nInput:[CONTEXT]{}[ENDOFDIALOGUE][QUESTION]The topic of conversation is\".format(user_input)\n",
    "tokens_input = inst_tokenizer(instruct_input, max_length=250, padding='max_length', truncation=True, return_tensors='pt')\n",
    "input_out = inst_model.generate(**tokens_input)\n",
    "topic = inst_tokenizer.decode(input_out[0], skip_special_tokens=True)\n",
    "print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00443188 0.08218222 0.91338587]\n",
      "1) positive 0.9134\n",
      "2) neutral 0.0822\n",
      "3) negative 0.0044\n"
     ]
    }
   ],
   "source": [
    "tokens_input = sent_tokenizer(user_input, max_length=250, padding='max_length', truncation=True, return_tensors='pt')\n",
    "input_out = sent_model(**tokens_input)\n",
    "\n",
    "scores = softmax(input_out[0][0].detach().numpy())\n",
    "print(scores)\n",
    "\n",
    "ranking = np.argsort(scores)\n",
    "ranking = ranking[::-1]\n",
    "for i in range(scores.shape[0]):\n",
    "    l = config.id2label[ranking[i]]\n",
    "    s = scores[ranking[i]]\n",
    "    print(f\"{i+1}) {l} {np.round(float(s), 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_topic_sentiment(text_in):\n",
    "    instruct_input = \"Instruction:What is the topic of conversation?\\n\\nInput:[CONTEXT]{}[ENDOFDIALOGUE][QUESTION]The topic of conversation is\".format(text_in)\n",
    "    tokens_input = inst_tokenizer(instruct_input, max_length=250, padding='max_length', truncation=True, return_tensors='pt')\n",
    "    input_out = inst_model.generate(**tokens_input)\n",
    "    topic = inst_tokenizer.decode(input_out[0], skip_special_tokens=True)\n",
    "\n",
    "    tokens_input = sent_tokenizer(text_in, max_length=250, padding='max_length', truncation=True, return_tensors='pt')\n",
    "    input_out = sent_model(**tokens_input)\n",
    "\n",
    "    scores = softmax(input_out[0][0].detach().numpy())\n",
    "    #print(scores)\n",
    "\n",
    "    ranking = np.argsort(scores)\n",
    "    ranking = ranking[::-1]\n",
    "    for i in range(scores.shape[0]):\n",
    "        l = config.id2label[ranking[i]]\n",
    "        s = scores[ranking[i]]\n",
    "        print(f\"{i+1}) {l} {np.round(float(s), 4)}\")\n",
    "\n",
    "    return topic, config.id2label[ranking[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CELL IS JUST USED FOR DATASET MANIPULATION\n",
    "# file = open('./CoT/generated_ds.csv', 'r')\n",
    "# f2 = open('./CoT/ds.txt', 'w')\n",
    "# lines = file.readlines()\n",
    "\n",
    "# for line in lines:\n",
    "#     idx = line.find(':')\n",
    "#     l2 = '[' + line[:idx]  + ']|' + line[idx+1:]\n",
    "#     f2.write(l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AI TOPIC GENERATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inst_tokenizer2 = AutoTokenizer.from_pretrained(\"prakharz/DIAL-BART0\")\n",
    "inst_model2 = AutoModelForSeq2SeqLM.from_pretrained(\"prakharz/DIAL-BART0\")\n",
    "\n",
    "inst_model2.load_state_dict(torch.load('./model/topic_er.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain of topics\n",
    "def generate_cot(text_in):\n",
    "    tok_text = inst_tokenizer2(text_in, return_tensors='pt')\n",
    "    gen_text = inst_model2.generate(**tok_text)\n",
    "    dec_text = inst_tokenizer2.decode(gen_text[0], skip_special_tokens=True)\n",
    "    return dec_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/trevor/SecondaryM2/VirginiaTech/HokieGuideliner/hokieguideliner/lib/python3.10/site-packages/transformers/generation/utils.py:1346: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) positive 0.7394\n",
      "2) neutral 0.2476\n",
      "3) negative 0.013\n",
      "('Abraham Lincoln', 'positive')\n",
      "[History, American Revolution, Presidents/Prime Ministers]\n"
     ]
    }
   ],
   "source": [
    "print(extract_topic_sentiment(\"I like Abraham Lincoln\"))\n",
    "print(generate_cot(\"Abraham Lincoln\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Animals': {'Marine Life': {'Sharks': {'Beach': {}}}}}\n",
      "{'Animals': {'Marine Life': {'Sharks': {'Beach': {}, 'Sand': {'relationship': 'dislike'}}}}}\n",
      "\n",
      "{'Animals': {'Marine Life': {'Sharks': {'Beach': {}, 'Sand': {'relationship': 'like'}}}}}\n",
      "{'Animals': {'Marine Life': {'Sharks': {'Beach': {}, 'Sand': {'relationship': 'like', 'strength': 5}}}}}\n"
     ]
    }
   ],
   "source": [
    "# META-TOPIC : {Music, Sports, Games}\n",
    "# SUB-TOPIC : {Rock n Roll, Country, etc.}\n",
    "# MICRO-TOPIC : {Journey, Boston, ACDC, Aerosmith}\n",
    "memory = {}\n",
    "\n",
    "def add_to_memory(mem, memory, idx, key=None, value=None):\n",
    "    # print(mem)\n",
    "    if len(mem) == 0:\n",
    "        #print(memory.keys())\n",
    "        if key != None:\n",
    "            if key in memory.keys():\n",
    "                memory[key] = value\n",
    "            else:\n",
    "                memory[key] = value\n",
    "\n",
    "        return memory\n",
    "    \n",
    "    if mem[0] not in memory.keys():\n",
    "        memory[mem[0]] = {}\n",
    "        \n",
    "        if key != None:\n",
    "            #print(mem[0])\n",
    "            #print(memory[mem[0]].keys())\n",
    "            if key not in memory[mem[0]].keys():\n",
    "                memory[mem[0]] = {key:value}\n",
    "\n",
    "    add_to_memory(mem[1:], memory[mem[0]], idx+1, key, value)\n",
    "    return memory\n",
    "\n",
    "print(add_to_memory(['Animals', 'Marine Life', 'Sharks', 'Beach'], memory, 0))\n",
    "print(add_to_memory(['Animals', 'Marine Life', 'Sharks', 'Sand'], memory, 0, 'relationship', 'dislike'))\n",
    "print()\n",
    "print(add_to_memory(['Animals', 'Marine Life', 'Sharks', 'Sand'], memory, 0, 'relationship', 'like'))\n",
    "print(add_to_memory(['Animals', 'Marine Life', 'Sharks', 'Sand'], memory, 0, 'strength', 5))\n",
    "\n",
    "# print(memory['Animals']['Marine Life']['Sharks']['Sand'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Retreive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Beach': {}, 'Sand': {'relationship': 'like', 'strength': 5}}\n"
     ]
    }
   ],
   "source": [
    "def read_memory(mem, memory, idx):\n",
    "    if len(mem) == 0:\n",
    "        return memory\n",
    "    return read_memory(mem[1:], memory[mem[0]], idx+1)\n",
    "\n",
    "print(read_memory(['Animals', 'Marine Life', 'Sharks'], memory, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node2Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Beach': {}, 'Sand': {'relationship': 'like', 'strength': 5}} <- IS A STRING\n"
     ]
    }
   ],
   "source": [
    "print(str(read_memory(['Animals', 'Marine Life', 'Sharks'], memory, 0))+\" <- IS A STRING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guideline Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE SAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE SAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Extraction 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) negative 0.9059\n",
      "2) neutral 0.0821\n",
      "3) positive 0.012\n",
      "('Non-fiction', 'negative')\n",
      "[Literature, Science Fiction, Novels]\n"
     ]
    }
   ],
   "source": [
    "# USE METHOD FROM MEMORY EXTRACTION 1\n",
    "ext = extract_topic_sentiment(\"I do not like reading non-fiction books.\")\n",
    "# WILL NEED TO RUN THIS SENTENCE BY SENTENCE?\n",
    "print(ext)\n",
    "\n",
    "cot = generate_cot(ext[0])\n",
    "print(cot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hokieguideliner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
